{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of The Project\n",
    "The aim of the project is to predict weather phenomena with multiple linear regression model. We are particularly interested in predicting the average temperatures. The predictions should be based on values such as rainfall, amount of UV, max and min temperatures. We found a good dataset containing weather data from airport in Perth, Australia. The dataset contains data from 1944 till 2016. With this dataset we set out to implement our models as best as we could.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Tutorial Used For Learning\n",
    "Before starting working on our own model. We decided to go through some tutorials and learn about linear regression. We found multiple good tutorials on the subject but after brief discussion we decided on one tutorial that we would do as a group. [The tutorial we used](https://towardsdatascience.com/a-beginners-guide-to-linear-regression-in-python-with-scikit-learn-83a8f7ae2b4f) taught us how to build a linear regression models with scikit-learn python library. The tutorial was divided in two parts: simple linear regression and multiple linear regression. Since our models was going to rely on multiple variables the second part was more useful for us. But the first part of the tutorial was also very helpful with examples of how matplotlib and pandas libraries work.\n",
    "\n",
    "In general the tutorial was of high quality and we can recommend it for everyone who is interested in building their own linear regression models or just are interested in learning about machine learning and the libraries used for machine learning in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing the dataset\n",
    "\n",
    "The original dataset being used is a CSV file, wa_weather_1944_till_2016.csv. We use the pandas library to read\n",
    "the contents of the file with read_csv() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('wa_weather_1944_till_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape\n",
    "\n",
    "We use shape on the dataset to return the amount of columns and rows. The dataset contains 26543 rows and 9 columns, that are: year, month, day, rainfall amount in mm, the min and max temperatures in celcius, the daily average temperature, the daily temperature range and finally the amount of ultraviolet light. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe\n",
    "\n",
    "The describe function gives an overview of our dataset and gives a detailed description of the differenct values of the dataset attributes, i.e. the total amount, mean, min and max values.\n",
    "* Year - Year of the measurement,\n",
    "* Month - Month of the measurement,\n",
    "* Day - Day of the measurement,\n",
    "* Rainfall - Amount of rain,\n",
    "* Min. temperature - Minimum temperature that day,\n",
    "* Max. Temperature - Maximum temperature that dat,\n",
    "* Daily avg. - Average temperature that day,\n",
    "* Daily range - Daily range of temperature that dat,\n",
    "* UV/MJ m*m - amount of UV radiation that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "We check if the dataset contains any null values and remove them using isnull().any() to return a boolean value and dropna() to drop the null values from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of results\n",
    "\n",
    "The algorithm predicted the results of the test data fairly accurately. A comparison can be seen in the table below which shows the actual values of the test data in left column and the predicted values in the other. As can be seen, the predicted values fall into range of the actual dataset test values.\n",
    "\n",
    "### Algorithm performance\n",
    "\n",
    "To get a better evaluation of the results we use evaluation metrics to determine the accuracy of the algorithm. The python Scikit-Learn library is used to calculate the root mean squared error, that is the square root of the mean of the squared errors. The calculation gives a result of 1.52 and comparing it to the daily_avg mean gives us a percentage.\n",
    "\n",
    "(1.52 / 18.22) * 100 = 8%\n",
    "\n",
    "This is a good enough value to get a fairly accurate prediction on the daily average temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T08:07:47.668598Z",
     "start_time": "2020-04-06T08:07:47.647206Z"
    }
   },
   "source": [
    "# Analysis process\n",
    "After we were decided on what dataset to use and what to predict with it we started by making a git repository and a common development environment. Our development environment is jupyter notebook running inside a conda virtual environment. This way it was easy for us to install and manage the libraries we wanted to use.\n",
    "\n",
    "The actual process for the analysis started by looking at our dataset and figuring out what to use it for. The nature of the data being about weather we wanted to try to predict the daily average temperature based on other attributes such as the amount of UV radiation, rainfall and taking into consideration the time of the year.\n",
    "With our goal set out in our minds we started preparing the data for our multiple linear regression model. By preparing the data we made sure that the dataset did not contain any bad values and our end result wouldn't be affected by them. Our method of cleaning up the data from these bad values meant dropping them from the dataset. Luckily there weren't many of such values.\n",
    "\n",
    "With our cleaned up data we were ready to make training and testing datasets. Before making the datasets we had to decide what variables to use in our model. For our purposes these variables were rainfall, uv radiation and max temperature. We used these variables to predict daily average temperature. Our datasets were split into 80% training data and 20% testing data. The training data was used for creating our multiple linear regression model and training it. As for the testing data it was used for testing the model.\n",
    "\n",
    "Once we had our datasets ready. The training datasets were fitted into a regression model. The model training method took the variables that know as their own dataset and the variable we want to predict as a seperate dataset. The model training function, in a nutshell, found us the best available line from the mess of datapoints and from that line we can make our predictions. With this method we got ourselves some coeffients that tells us how the variables affect the value we are trying to predict, in this case the daily average temperature. Now with our very own model we tried to predict some values with the testing dataset. We made our predictions and compared them to the testing dataset. By comparing them against each other we were able to determine how good our predictions were."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
